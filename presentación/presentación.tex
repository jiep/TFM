%!TeX spellcheck = es_ES
%!TEX program = lualatex
\documentclass[hyperref={unicode}]{beamer}

\usepackage[spanish, es-tabla, es-nodecimaldot]{babel}
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\usepackage{bookmark}
\usepackage{tikz}
\usepackage{etoolbox}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usetikzlibrary{spy}


\input{../memoria/extras/figuras}

\newtheorem{teo}{\textbf{\color{ExecusharesBlue}Teorema}}
\newtheorem{defi}{\textbf{\color{ExecusharesBlue}Definición}}

\usetheme{TFM}

\title{Machine Learning clásico y Redes Parenclíticas. Un análisis comparativo}
\subtitle{Trabajo Fin de Máster \\ \textbf{Máster en Ingeniería de Sistemas de Decisión} \\ \textit{Curso 2016--2017}}
\author{José Ignacio Escribano Pablos}
\institute{\begin{tabular}{c}
Ana Elizabeth García Sipols \\
Miguel Romance del Río     
\end{tabular}}
\titlegraphic{\includegraphics[width=2cm]{imagenes/urjc.png}}
\date{19 de enero de 2017}

\setcounter{tocdepth}{2}

\makeatletter
\patchcmd{\beamer@sectionintoc}
{\vfill}
{\vskip\itemsep}
{}
{}
\makeatother  


\setcounter{showSlideNumbers}{1}

\begin{document}
\setcounter{showProgressBar}{0}
\setcounter{showSlideNumbers}{0}

\frame{\titlepage}

\begin{frame}{Índice}
	\tableofcontents
\end{frame}

\setcounter{framenumber}{0}
\setcounter{showProgressBar}{1}
\setcounter{showSlideNumbers}{1}

\begin{frame}{Objetivos}
	\begin{enumerate}
		\item Introducir los conceptos básicos de Machine Learning, teoría de grafos y redes parenclíticas (basadas en grafos).
		
		\item Desarrollar una aplicación que automatice el proceso del aprendizaje según los distintos algoritmos de Machine Learning anteriores.
		
		\item Analizar una base de datos y comparar los resultados de los distintos algoritmos de Machine Learning.
	\end{enumerate}
\end{frame}

\section{Introducción}
\begin{frame}{Introducción}
	\begin{figure}
			\begin{center}
			\includegraphics[width=0.8\textwidth]{imagenes/introduccion}
			\caption{Predicción de datos almacenados en 2025. Fuente:~\url{http://www.nanalyze.com/2015/12/investing-in-the-5-biggest-data-storage-companies/}}
			\end{center}
		\end{figure}
\end{frame}

\section{Introducción al Machine Learning}
\begin{frame}{Definición de Machine Learning}
	\begin{fancyquotes}
		Machine learning, in artificial intelligence (a subject within computer science), discipline concerned with the implementation of computer software that can learn autonomously.
	\end{fancyquotes}

	\ \\

	\begin{figure}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{imagenes/ml.png}
			\caption{Esquema de funcionamiento del Machine Learning. Fuente~\url{http://phdp.github.io/images/ml-900.png}}
		\end{center}
	\end{figure}
\end{frame}

\subsection{Problemas que resuelve}
\begin{frame}{Problemas que resuelve}
El Machine Learning permite resolver infinidad de problemas que se pueden clasificar en:

%\begin{itemize}
%\item Clasificación
%\item Regresión
%\item Clustering
%\item Reducción de la dimensionalidad
%\end{itemize}

\begin{figure}
	\centering
	\subfloat[Clasificación]{\includegraphics[width=0.40\textwidth]{imagenes/clasificacion1.png}}\qquad\qquad
	\subfloat[Regresión]{\includegraphics[width=0.40\textwidth]{imagenes/regresion1.png}}\\
	\subfloat[Clustering]{\includegraphics[width=0.40\textwidth]{imagenes/clustering.png}}\qquad\qquad
	\subfloat[Reducción dimensionalidad]{\includegraphics[width=0.22\textwidth]{imagenes/dimesionality_reduction.png}}
\end{figure}

\end{frame}

\subsection{Tipos de aprendizaje}
\begin{frame}{Tipos de aprendizaje}
	Se pueden clasificar tres tipos de aprendizaje en Machine Learning:
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=0.6\textwidth]{imagenes/machine-learning-types.png}
			\caption{Tipos de Machine Learning según el aprendizaje. Fuente:~\url{https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics}}
		\end{center}
	\end{figure}
\end{frame}

\subsubsection{Aprendizaje supervisado}
\begin{frame}{Aprendizaje supervisado}
	\begin{minipage}[b][0.7\textheight][t]{.60\textwidth}
		\begin{figure}
			\begin{center}
				\includegraphics[width=\textwidth]{imagenes/supervisado.jpg}
				\caption{Esquema del aprendizaje supervisado. Fuente~\url{https://s-media-cache-ak0.pinimg.com/originals/5f/06/5b/5f065b7604e12bb776f9323bb29d33cf.jpg}}
			\end{center}
		\end{figure}
	\end{minipage}
	\begin{minipage}[b][0.7\textheight][t]{.37\textwidth}
		\textit{Proceso de Machine Learning supervisado.}
		\begin{enumerate}
			\item Obtención de datos y preparación.
			\item Selección de características.
			\item Elección del algoritmo.
			\item Selección del modelo y sus parámetros.
			\item Entrenamiento.
			\item Evaluación.
		\end{enumerate}
	\end{minipage}
	
\end{frame}

\begin{frame}{Algunos algoritmos}
	\begin{itemize}
		\item Redes Neuronales Artificiales
			\item Support Vector Machine (SVM)
			\item Árboles de decisión
			\item k-vecinos más cercanos (kNN)
			\item Regresión lineal
			\item Regresión logística
			\item ...
	\end{itemize}
\end{frame}



\subsection{Técnicas clásicas consideradas}
\begin{frame}{Redes neuronales artificiales}
	Las redes neuronales están basadas en el modo que funcionan las neuronas en el cerebro.\\
	
	
	\begin{figure}[htbp!]
	\centering
	\resizebox{!}{0.5\width}{\neuronaMcCullochPitts}
		
	\caption{Neurona de McCulloch y Pitts}
	\end{figure}
\end{frame}

\begin{frame}{Perceptrón}

El \textbf{perceptrón} es la red neuronal más sencilla, ya que no es más que una colección de neuronas de MacCulloch y Pitts

\begin{figure}[htbp!]
	\centering
	\resizebox{!}{0.35\textwidth}{\perceptron}
	\caption{Perceptrón}
\end{figure}

Sólo es capaz de clasificar conjuntos linealmente separables.
	
\end{frame}



\begin{frame}{Perceptrón multicapa}
 
El \textbf{perceptrón multicapa} consiste en múltiples capas de neuronas.

\begin{figure}[htbp!]
	\centering
	\resizebox{!}{0.3\textwidth}{\perceptronmulticapa}
	\caption{Perceptrón multicapa}
\end{figure}

\begin{teo}[de aproximación universal]
	Un perceptrón multicapa con una sola capa oculta puede aproximar funciones continuas en subconjuntos compactos de $\mathbb{R}^n$.
\end{teo}

\end{frame}

\begin{frame}{Support Vector Machine (SVM)}
	
	\begin{minipage}[b][.50\textheight][t]{.47\textwidth}
		\begin{figure}[htbp!]
			\centering
			\resizebox{!}{0.8\textwidth}{\clasificador}
			\caption{Distintos clasificadores lineales}
			\label{fig:clasificador}
		\end{figure}
	\end{minipage}\hfill%
	\begin{minipage}[b][.50\textheight][t]{.47\textwidth}
		\begin{figure}[htbp!]
			\centering
			\resizebox{!}{0.9\textwidth}{\margen}
			\caption{Margen en SVM}
			\label{fig:margen}
		\end{figure}
	\end{minipage}
	
	
\end{frame}


\begin{frame}{Support Vector Machine (SVM)}
	El problema anterior se puede plantear como un problema de programación cuadrática con la siguiente estructura:
	
	\begin{align}
	 \min & \ \ \ \dfrac{1}{2}\mathbf{w}^T \mathbf{w}\\
	 \text{s.a. } & \ \ \ y_i (\mathbf{w}^T x_i + b) \geq 1 \ \forall i = 1,\dots, n
	 \end{align}
	 
	 donde $\mathbf{x}$ es el vector de entradas, $\mathbf{w}$ es un vector perpendicular al hiperplano clasificador, $b$ es una constante y $y_i$ es el valor de la salida i-ésima que puede  tomar dos valores $\{-1,1\}$. 
	 
\end{frame}

\begin{frame}{Árboles de decisión}
	La idea de los árboles de decisión es partir el conjunto de clasificación en un conjunto de opciones sobre cada variable comenzando por la raíz del árbol y bajando hasta las hojas, donde se reciben la decisión de clasificación.
	
	\begin{figure}[htbp!]
		\centering
			\resizebox{!}{0.3\textwidth}{\arboldedecision}
		\caption{Árbol de decisión}
		\label{fig:arboldecision}
	\end{figure}
\end{frame}

\begin{frame}{Algunos algoritmos}
	\begin{minipage}{0.2\textwidth}
	\begin{itemize}
		\item ID3
		\item C4.5
		\item CART
		\item CHAID
		\item ...
		\end{itemize}
	\end{minipage}
	\hfill
	\begin{minipage}{0.7\textwidth}
		\begin{figure}[htbp!]
				\centering
				
				\resizebox{!}{0.5\textwidth}{\ejemploarboldecision}
				\caption{Árbol de decisión tras la ejecución del algoritmo ID3}
				\label{fig:arboldecisionejemplo}
			\end{figure}
	\end{minipage}
\end{frame}

\subsubsection{Aprendizaje no supervisado}
\begin{frame}{Aprendizaje no supervisado}
	\begin{figure}
			\begin{center}
			\includegraphics[width=0.9\textwidth]{imagenes/no_supervisado.jpg}
			\caption{Esquema de aprendizaje no supervisado. Fuente:~\url{https://s-media-cache-ak0.pinimg.com/originals/51/c0/b4/51c0b4e42094987971be43849084c6d9.jpg}}
			\end{center}
	\end{figure}
\end{frame}

\begin{frame}{Algunos algoritmos}

\begin{minipage}{0.3\textwidth}
	\begin{itemize}
		\item k-Medias
		\item Clustering jerárquico
		\item ...
	\end{itemize}
\end{minipage}
\hfill
\begin{minipage}{0.65\textwidth}
	\begin{figure}[htbp!]
		\centering
		\resizebox{0.3\textheight}{!}{\includegraphics{imagenes/iris_dendrogram.png}}
		\caption{Clustering jerárquico. Fuente:~\url{https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html}}
	\end{figure}
\end{minipage}
	
\end{frame}

\subsubsection{Aprendizaje por refuerzo}
\begin{frame}{Aprendizaje por refuerzo}
	
	\begin{figure}[htbp!]
			\centering
			\resizebox{!}{0.5\textwidth}{\includegraphics{imagenes/reinforcement_learning.png}}
			\caption{Esquema del aprendizaje por refuerzo. Fuente:~\url{https://goo.gl/PdJ0c2}}
		\end{figure}
\end{frame}

\begin{frame}{Algunos algoritmos}
	\begin{minipage}{0.3\textwidth}
		\begin{itemize}
			\item Q-learning
			\item SARSA
			\item ...
		\end{itemize}
	\end{minipage}
	\hfill
	\begin{minipage}{0.65\textwidth}
		\begin{figure}[htbp!]
			\centering
			\resizebox{0.45\textheight}{!}{\includegraphics{imagenes/breakout.png}}
			\caption{Aplicación del algoritmo Q-learning al juego Breakout. Fuente:~\url{https://youtu.be/V1eYniJ0Rnk}}
		\end{figure}
	\end{minipage}
\end{frame}

\section{Redes parenclíticas}
\subsection{Introducción a la teoría de grafos}
\begin{frame}{Introducción a la teoría de grafos}
	\begin{defi}
		Un grafo (o grafo no dirigido) es un par $G = (V,E)$ de conjuntos que satisfacen que $E \subseteq V\times V$ y $V \cap E = \emptyset$. Los elementos de $V$ se denominan vértices (o nodos) del grafo $G$ y los elementos de $E$ se denominan arcos (o aristas). Una arista entre los vértices $x, y \in V$ se denota como $xy$ o $yx \in E$.
	\end{defi}
	
	\begin{figure}[htb]
		\centering
		\resizebox{0.25\textheight}{!}{\ejemplografo}
		\caption{Ejemplo de grafo}
		\label{fig:grafo}
	\end{figure}
\end{frame}

\subsection{Medidas de redes complejas}
\begin{frame}{Medidas de redes complejas}
	
	\begin{minipage}[b][.35\textheight][t]{.47\textwidth}
		\begin{defi}
			Se define densidad de enlaces del grafo $G$ como el número de enlaces dividido entre el número de enlaces que pueden estar presentes en el grafo.
		\end{defi}
	\end{minipage}\hfill%
	\begin{minipage}[b][.35\textheight][t]{.47\textwidth}
		\begin{defi}
			Se define longitud del camino característico del grafo $G$ como
			
			\begin{equation}\label{eq:camino}
			\mathrm{L}({G}) = \dfrac{1}{\binom{n}{2}} \sum_{i\neq j} d(i,j)
			\end{equation}
		\end{defi}
	\end{minipage}\\[0.5em]
	\begin{minipage}[b][.35\textheight][t]{.47\textwidth}
		\begin{defi}
			Se define eficiencia de un grafo $G$ como
			
			\begin{equation}\label{eq:eficiencia}
			\mathrm{E}({G}) = \dfrac{1}{\binom{n}{2}} \sum_{i\neq j} \dfrac{1}{d(i,j)}
			\end{equation}
			
		\end{defi}
	\end{minipage}\hfill
	\begin{minipage}[b][.35\textheight][t]{.47\textwidth}
		\begin{defi}
			Se define coeficiente de clustering $C$ del grafo $G$ como
			
			\begin{equation}\label{eq:clustering}
			C(G) = \dfrac{3N_{\triangle}}{N_3}
			\end{equation}
		\end{defi}
	\end{minipage}%
\end{frame}

\subsection{Descripción del método de redes parenclíticas}
\begin{frame}{Método de redes parenclíticas}
	\begin{itemize}
	\item Aprendizaje supervisado. Problema de clasificación.
	\item Variables numéricas (excepto la de clasificación).
	\item \textbf{Idea}: asociar a cada observación un grafo y calcular sobre ellos medidas de redes complejas para realizar la clasificación. 
	\end{itemize}
	
	\begin{figure}[htbp!]
		\begin{center}
			\resizebox{\textwidth}{!}{%
				\redesparencliticas
			}
		\end{center}
		\caption{Esquema del método de redes parenclíticas}
		\label{fig:redesparencliticas}
	\end{figure}
\end{frame}


\begin{frame}{Método de redes parenclíticas}
	\begin{figure}[htbp!]
		\centering
		\includegraphics[width=0.8\textwidth]{imagenes/parenclitic-networks.jpg}
		\label{fig:parencliticas}
		\caption{Construcción de una red parenclítica. \\
		Fuente: Zanin M, Medina J, Carbajosa JV, Paez M, Papo D, Sousa P, Menasalvas E, Boccaletti S (2014). Parenclitic networks: uncovering new functions in biological data. Nature Scientific reports 4, 5112.}
	\end{figure} 
\end{frame}


\section{Diseño de la aplicación}
\begin{frame}{Arquitectura y tecnología}
	\begin{figure}[htbp!]
		\centering
		\arquitectura
		\caption{Arquitectura de la aplicación}
		\label{fig:arquitectura}
	\end{figure}
\end{frame}


\begin{frame}{Vista de la aplicación}
	\begin{center}
	\Huge \color{ExecusharesBlue}\textbf{\{DEMO\}}
	\end{center}
	\begin{figure}[htbp!]
		\centering
		\href{https://parencliticnetworks.shinyapps.io/ParencliticsNetworks}{\includegraphics[width=0.7\textwidth]{../memoria/imagenes/descriptiva.png}}
		\label{fig:aplicacion}
	\end{figure}
\end{frame}



\section{Aplicación al cáncer de mama}
\begin{frame}{Datos utilizados}
	699 observaciones y 11 variables (con valores entre 1 y 10):
	\begin{enumerate}
		\item Código de la muestra
		\item Espesor
		\item Uniformidad del tamaño celular
		\item Uniformidad de la forma celular
		\item Adhesión marginal
		\item Tamaño de las células epiteliales
		\item Bare nuclei
		\item Cromatina
		\item Normal Nucleoli
		\item Mitosis
		\item Clase (B = beningno; M = maligno)
	\end{enumerate}
\end{frame}

\begin{frame}{Resultados}
\begin{figure}[htbp!]
	\centering
	\subfloat[Obs. 8]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/b1.png}}
	\subfloat[Obs. 26]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/b2.png}}
	\subfloat[Obs. 34]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/b3.png}}\\
	\subfloat[Obs. 41]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/m1.png}}
	\subfloat[Obs. 43]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/m2.png}}
	\subfloat[Obs. 50]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/m3.png}}
	\caption{Redes parenclíticas por clase de tumor con modelo de regresión lineal}
\end{figure}
\end{frame}


\begin{frame}{Resultados}
\begin{figure}[htbp!]
	\centering
	\subfloat[Obs. 8]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/b1_reciproco.png}}
	\subfloat[Obs. 26]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/b2_reciproco.png}}
	\subfloat[Obs. 34]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/b3_reciproco.png}}\\
	\subfloat[Obs. 41]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/m1_reciproco.png}}
	\subfloat[Obs. 43]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/m2_reciproco.png}}
	\subfloat[Obs. 50]{\includegraphics[width=0.25\textwidth]{../memoria/imagenes/cancer/m3_reciproco.png}}	
	\caption{Redes parenclíticas por clase de tumor con modelo de regresión recíproco}
\end{figure}
\end{frame}

\begin{frame}{Resultados}
\begin{figure}
\begin{center}
			\includegraphics[width=\linewidth]{../memoria/imagenes/cancer/ROC_cancer.png}
		\end{center}
	\caption{Curvas ROC}
\end{figure}
\end{frame}

\begin{frame}{Umbralización}
	\begin{figure}[htbp!]
		\centering	
		\resizebox{!}{0.5\textwidth}{\ejemploumbralizacion}
		\caption{Ejemplo de umbralización de una red parenclítica}
		\label{fig:ejemploumbralizacion}
	\end{figure}
\end{frame}

\begin{frame}{Resultados}
	\begin{figure}[htbp!]
		\centering
		\includegraphics[width=0.7\textwidth]{../memoria/imagenes/cancer/umbralizacion.png}
		\caption{Resultados de la umbralización}		\label{fig:umbralizacion-resultados}
	\end{figure}
\end{frame}

\section{Conclusiones y trabajo futuro}
\begin{frame}{Conclusiones}
	\begin{itemize}
	\item Hemos visto qué es el Machine Learning, qué tipos existen según el tipo de aprendizaje y algunos de los algoritmos clásicos y un nuevo método basado en teoría de grafos para realizar la tarea de clasificación. 
	
	\item Hemos desarrollado una aplicación que analiza y compara un conjunto de datos según distintos algoritmos de Machine Learning.
	
	\item Hemos utilizado la aplicación al diagnóstico de tumores de cáncer de mama, obteniendo grandes resultados, aunque sin llegar al nivel de los algoritmos clásicos.
	\end{itemize}
\end{frame}

\begin{frame}{Trabajo futuro}
	\begin{itemize}
		\item Implementación de un módulo de validación.
		
		\item Optimización de los algoritmos de redes parenclíticas.
		 
		\item Mejora en visualización de la aplicación.
		
		\item Nuevos métodos de regresión.
		
		\item Persistencia de las redes parenclíticas en bases de datos de grafos.
	\end{itemize}
\end{frame}

\begin{frame}{¿Preguntas?}
	\begin{figure}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{imagenes/questions}
	\end{center}

	\end{figure}


\end{frame}

\end{document}

\begin{frame}{Regresión lineal}
	La regresión lineal viene dada por el modelo
	
	\begin{equation}
	y_i = \alpha_1 x_{i1} + \dots + \alpha_p x_{ip} + \alpha_0, \quad i = 1,...,n
	\end{equation}
	
	donde $y_i$ es la variable salida, $x_{i}$ es la variable de entrada, $n$ es el número de observaciones y $p$ es el número de variables de entrada.
	
	\begin{figure}
		\centering
		\subfloat[Dos dimensiones]{\includegraphics[width=0.3\textwidth]{imagenes/linear_regression2d.png}}\qquad\qquad
		\subfloat[Tres dimensiones]{\includegraphics[width=0.3\textwidth]{imagenes/linear_regression.png}}
		
	\end{figure}
\end{frame}


\begin{frame}{Regresión lineal (bidimensional)}
	En particular, en el caso bidimensional el modelo viene dado por
	
	\begin{equation}
	y = \alpha_1 x + \alpha_0.
	\end{equation}
	
	Se tiene que $\alpha_0$ y $\alpha_1$ vienen dados por este sistema de ecuaciones lineales:
	
	\begin{equation}
	\begin{cases}
	\begin{array}{ccccc}
	\left(\displaystyle\sum_{i=1}^{n} x_i^2\right) \alpha_1 & + & \left(\displaystyle\sum_{i=1}^{n} x_i\right) \alpha_0 & = & \displaystyle \sum_{i=1}^{n} x_i y_i \\
	\left(\displaystyle \sum_{i=1}^{n} x_i\right) \alpha_1 & + & n \alpha_0 & = & \displaystyle \sum_{i=1}^{n} y_i
	\end{array}
	\end{cases}
	\end{equation}
	donde $n$ es el número total de datos.
\end{frame}


\begin{frame}{Linealización de modelos no lineales}
	\begin{table}[htbp!]
		\centering
		\caption{Linealización de distintos modelos}
		\label{tbl:linealizacion}
		\resizebox{\textwidth}{!}{%
			\begin{tabular}{@{}ccc@{}}
				\toprule
				$y = f(x)$                             & \begin{tabular}[c]{@{}c@{}}Forma linealizada\\ $y = \alpha_1 x + \alpha_0$\end{tabular} & \begin{tabular}[c]{@{}c@{}}Cambio de variables\\ y constantes\end{tabular} \\ \midrule
				$y = \dfrac{\alpha_1}{x} + \alpha_0$   & $y = \alpha_1 \dfrac{1}{x} + \alpha_0$                                                  & $X=\dfrac{1}{x}$; $Y=y$                                                    \\
				\addlinespace[1em]
				$y = \dfrac{1}{\alpha_1 x + \alpha_0}$ & $\dfrac{1}{y} = \alpha_1 x + \alpha_0$                                                  & $Y=\dfrac{1}{y}$; $X=x$                                                    \\
				\addlinespace[1em]
				$y = \alpha_1 \log x + \alpha_0$       & $y = \alpha_1 \log x + \alpha_0$                                                        & $Y = y$; $X = \log x$                                                      \\
				\addlinespace[1em]
				$y = \alpha_1 e^{\alpha_0 x}$          & $\log y = \log \alpha_1 + \alpha_2 \log x$                                              & $Y = \log y$; $X = \log x$; $\alpha_1 = \log \alpha_1$                     \\
				\addlinespace[1em]
				$y = (\alpha_0 + \alpha_1 x)^2$        & $\sqrt{y} = \alpha_0 + \alpha_1 x$                                                      & $Y = \sqrt{y}$; $X=x$                                                      \\ \bottomrule
			\end{tabular}
		}
	\end{table} 
\end{frame}

